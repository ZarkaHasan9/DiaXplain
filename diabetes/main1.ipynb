{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2954608a-c0c8-498a-a26d-e97744e3e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2839aaf1-996d-4360-ac90-ab9514a356c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bb6f126-41bb-4542-b27c-fbfd21b11ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f539c43-c902-4e30-96be-bd1dd8ea5636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daaf3ac5-f5f4-4fc5-96d8-00bb0d4fd582",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "df[zero_cols] = df[zero_cols].replace(0, np.nan)\n",
    "\n",
    "df[zero_cols] = df[zero_cols].fillna(df[zero_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "152f9756-13df-4bda-a819-81572ccacb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQR applied to Pregnancies\n",
      "IQR applied to Glucose\n",
      "Z-score applied to BloodPressure\n",
      "IQR applied to SkinThickness\n",
      "IQR applied to Insulin\n",
      "IQR applied to BMI\n",
      "IQR applied to DiabetesPedigreeFunction\n",
      "IQR applied to Age\n",
      "IQR applied to Outcome\n"
     ]
    }
   ],
   "source": [
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def is_skewed(series, threshold=0.5):\n",
    "    return abs(series.skew()) > threshold\n",
    "\n",
    " \n",
    "def handle_outliers_zscore(data, column, threshold=3):\n",
    "    z_scores = zscore(data[column])\n",
    "    outliers = (z_scores > threshold) | (z_scores < -threshold)\n",
    "    data.loc[outliers, column] = data[column].median()\n",
    "    return data\n",
    " \n",
    "def handle_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "    data.loc[outliers, column] = data[column].median()\n",
    "    return data\n",
    "\n",
    " \n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "for column in numerical_columns:\n",
    "    if is_skewed(df[column]):   # if skewed\n",
    "        df = handle_outliers_iqr(df, column)\n",
    "        print(f\"IQR applied to {column}\")\n",
    "    else:                       # if approximately normal\n",
    "        df = handle_outliers_zscore(df, column)\n",
    "        print(f\"Z-score applied to {column}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0e0256a-00b1-4b89-ad9e-6602f60631d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\zarka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\zarka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\zarka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\zarka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\zarka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\zarka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\zarka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\zarka\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a60c1b93-7386-42d2-80f2-dbe3f152c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0b72cf5-9702-4381-9d92-d9d913068458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8eb04d92-46b1-47af-8e78-cd923f5928b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models1 = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
    "    \"Linear DA\": LinearDiscriminantAnalysis(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100,random_state=42),\n",
    "    \"Extra Tree\": ExtraTreesClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(eval_metric=\"logloss\"),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(min_gain_to_split=0.01  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2f5ccd4-68de-48b4-9de8-3d47e5dd2220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_gain_to_split is set=0.01, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01\n",
      "[LightGBM] [Info] Number of positive: 399, number of negative: 401\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 868\n",
      "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498750 -> initscore=-0.005000\n",
      "[LightGBM] [Info] Start training from score -0.005000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "trained_models = {}\n",
    "\n",
    "for name, model in models1.items():\n",
    "    model.fit(X_train_scaled, y_train)  # training on full training data\n",
    "    trained_models[name] = model       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "986c2159-84cc-4b5b-89a0-e48857f1f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_gain_to_split is set=0.01, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01\n",
      "                     Model  Accuracy  Precision    Recall  F1-Score\n",
      "0      Logistic Regression     0.750   0.742857  0.772277  0.757282\n",
      "1                Linear DA     0.745   0.740385  0.762376  0.751220\n",
      "2      K-Nearest Neighbors     0.730   0.694215  0.831683  0.756757\n",
      "3            Decision Tree     0.745   0.731481  0.782178  0.755981\n",
      "4            Random Forest     0.810   0.764706  0.900990  0.827273\n",
      "5               Extra Tree     0.810   0.756098  0.920792  0.830357\n",
      "6   Support Vector Machine     0.775   0.741379  0.851485  0.792627\n",
      "7        Gradient Boosting     0.765   0.732759  0.841584  0.783410\n",
      "8                 AdaBoost     0.780   0.766355  0.811881  0.788462\n",
      "9              Naive Bayes     0.720   0.714286  0.742574  0.728155\n",
      "10                 XGBoost     0.760   0.734513  0.821782  0.775701\n",
      "11                LightGBM     0.780   0.752212  0.841584  0.794393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zarka\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)  # Do NOT fit again on test data\n",
    "\n",
    "# Evaluate all trained models on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "results_list = []\n",
    "for name, model in trained_models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results_list.append([name, accuracy, precision, recall, f1])\n",
    "\n",
    "results = pd.DataFrame(results_list, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "print(results)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b06a3d6-44a8-4e77-8b70-b0170ce40e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted Model Performance:\n",
      "                      Model  Accuracy  Precision    Recall  F1-Score\n",
      "5               Extra Tree     0.810   0.756098  0.920792  0.830357\n",
      "4            Random Forest     0.810   0.764706  0.900990  0.827273\n",
      "11                LightGBM     0.780   0.752212  0.841584  0.794393\n",
      "6   Support Vector Machine     0.775   0.741379  0.851485  0.792627\n",
      "8                 AdaBoost     0.780   0.766355  0.811881  0.788462\n",
      "7        Gradient Boosting     0.765   0.732759  0.841584  0.783410\n",
      "10                 XGBoost     0.760   0.734513  0.821782  0.775701\n",
      "0      Logistic Regression     0.750   0.742857  0.772277  0.757282\n",
      "2      K-Nearest Neighbors     0.730   0.694215  0.831683  0.756757\n",
      "3            Decision Tree     0.745   0.731481  0.782178  0.755981\n",
      "1                Linear DA     0.745   0.740385  0.762376  0.751220\n",
      "9              Naive Bayes     0.720   0.714286  0.742574  0.728155\n",
      "Best model selected: Extra Tree\n"
     ]
    }
   ],
   "source": [
    "# Sort results by F1-Score (or any other preferred metric)\n",
    "results_sorted = results.sort_values(by=\"F1-Score\", ascending=False)\n",
    "print(\"\\nSorted Model Performance:\\n\", results_sorted)\n",
    "\n",
    "# Select the best model based on F1-Score\n",
    "best_model_name = results_sorted.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"Best model selected: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cde4d038-a1df-4b1f-8065-a28eb2fe002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved as 'best_model.pkl'!\n",
      "Scaler saved!\n",
      "Scaled X_train saved for LIME!\n",
      "Feature names saved!\n",
      "Class names saved!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.makedirs(\"static\", exist_ok=True)\n",
    " \n",
    "joblib.dump(best_model, \"static/best_model.pkl\")\n",
    "print(\"Best model saved as 'best_model.pkl'!\")\n",
    " \n",
    "joblib.dump(scaler, \"static/scaler.pkl\")\n",
    "print(\"Scaler saved!\")\n",
    "\n",
    "joblib.dump(X_train_scaled, \"static/xtrain_scaled.pkl\")\n",
    "print(\"Scaled X_train saved for LIME!\")\n",
    "\n",
    "feature_names = X.columns.tolist()  \n",
    "joblib.dump(feature_names, \"static/feature_names.pkl\")\n",
    "print(\"Feature names saved!\")\n",
    " \n",
    "class_names = ['Non-Diabetic', 'Diabetic']\n",
    "joblib.dump(class_names, \"static/class_names.pkl\")\n",
    "print(\"Class names saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
